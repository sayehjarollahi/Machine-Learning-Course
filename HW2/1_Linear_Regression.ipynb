{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16adc323",
   "metadata": {
    "id": "16adc323"
   },
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Linear Regression\n",
    "    </div>\n",
    "<br><br>\n",
    "<font size=4>\n",
    "   **Name**: Sayeh Jarollahi <br> \n",
    "   **Student ID**: 98101339 <br> <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585264a",
   "metadata": {
    "id": "2585264a"
   },
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 1: Linear Regression Model (40 + 30 optional points)\n",
    "According to <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Classical_Models/Linear%20regression.ipynb\"><font face=\"Roboto\">Linear Regression Notebook</font></a>, train a linear regression model on an arbitrary dataset. Explain your chosen dataset and split your data into train and test sets, then predict values for the test set using your trained model. Try to find the best hyperparameters for your model. (Using Lasso Regression, Ridge Regression or Elastic Net and comparing them will have extra optional points)\n",
    "<br> Explain each step of your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59b4fe",
   "metadata": {
    "id": "af59b4fe"
   },
   "source": [
    "<div dir = \"rtl\">\n",
    "<font size=\"4\">\n",
    "    در ابتدا دیتاست مربوط به مسئله را میسازیم. اینکار با استفاده از  sklearn انجام میشود.\n",
    "    <br>\n",
    "    پس از آن با استفاده از gradiant descent\n",
    "    و در ایتریشن های متوالی مقدار بهینه وزن استفاده میشود. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387d3a07",
   "metadata": {
    "id": "387d3a07"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hefypc9y2r2m",
   "metadata": {
    "id": "hefypc9y2r2m"
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=50_000, n_features=3, noise=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5cb104",
   "metadata": {
    "id": "9d5cb104"
   },
   "outputs": [],
   "source": [
    "class ZeroRegularizer:\n",
    "    \n",
    "    def regularize(self, w):\n",
    "        return 0\n",
    "    \n",
    "    def get_reg_cost(self, w):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a86aca5",
   "metadata": {
    "id": "9a86aca5"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, n_feats, regularization):\n",
    "        self.W = np.zeros(n_feats)\n",
    "        self.bias = 0\n",
    "        self.learning_rate = 0.1\n",
    "        self.iterations = 2000\n",
    "        self.regularization = regularization\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        self.bias = 0 \n",
    "        for i in range(self.iterations):\n",
    "            y_hat = np.dot(X, self.W) +self.bias\n",
    "            #db = (1/X.shape[0]) * np.sum(y_hat-y) \n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (y_hat-y)) + self.regularization.regularize(self.W)\n",
    "            self.W = self.W - self.learning_rate * dw\n",
    "            #self.bias = self.bias - self.learning_rate * db\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.W) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de5fc9c",
   "metadata": {
    "id": "8de5fc9c"
   },
   "outputs": [],
   "source": [
    "def test_model(X, y, regularizer, test_size=0.2, random_state=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size, random_state= random_state)\n",
    "\n",
    "    linreg = LinearRegression(X.shape[1], regularizer)\n",
    "    linreg.fit(X_train, y_train)\n",
    "\n",
    "    predictions = linreg.predict(X_test)\n",
    "    print(\"r2 score\", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d063de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51d063de",
    "outputId": "90d029c2-57f1-40f1-a378-7e1ba2811513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.9645790402026584\n"
     ]
    }
   ],
   "source": [
    "test_model(X, y, ZeroRegularizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c5ce6",
   "metadata": {
    "id": "eb3c5ce6"
   },
   "source": [
    "<div dir = \"rtl\">\n",
    "<font size=\"4\">\n",
    "    مشابه با آنچه در کلاس تدریس شده، کد مربوط به linear regression\n",
    "    پیاده سازی شده است. ابتدا داده اصلی را به دو بخش تست و ترین تقسیم میکنیم. 80 درصد از کل دیتا، مربوط به دیتای ترین و بقیه مربوط به دیتای تست خواهد بود. پس از آن با استفاده از فیت کردن داده ترین روی مدل، آن را ترین میکنیم. در هربار iteration\n",
    "    ابتدا تخمین ورودی را به دست می آوریم و اختلاف آن را با مقدار واقعی در نظر میگیریم. با استفاده از محاسبه گرادیان به کمک MSE\n",
    "    وزن و بایاس را آپدیت میکنیم. \n",
    "    <br>\n",
    "    در انتها مقدار ارور را بر روی داده تست محاسبه میکنیم.\n",
    "    توجه: برای آنکه از مدل بالا بتوانیم در بخش های آینده نیز استفاده کنیم، یک عبارت regularization\n",
    "    به آن اضافه کردیم که در صورتی که نرمال سازی نداشته باشیم مقدار صفر را دارد و در غیر اینصورت overwrite \n",
    "    یک کلاس است که در آن با کمک تابع regularize\n",
    "    مقدار را اضافه میکنیم. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753d930",
   "metadata": {
    "id": "b753d930"
   },
   "source": [
    "<div dir = \"rtl\">\n",
    "<font size=\"5\">\n",
    "  <b> \n",
    "      نرمال سازی با کمک Lasso\n",
    "      :\n",
    "  </b>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec37e",
   "metadata": {
    "id": "ab2ec37e"
   },
   "source": [
    "Lasso Regression uses L1-regularization norm to prevent the model from overfitting. We define the regularization factor. After that, normal linear regression can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba698fa",
   "metadata": {
    "id": "fba698fa"
   },
   "outputs": [],
   "source": [
    "class LassoRegularizer:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "    \n",
    "    def regularize(self, w):\n",
    "        return self.l * np.sign(w) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c374bce4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c374bce4",
    "outputId": "e48c9418-c5b1-44a0-e33b-68f4eeee5218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.9645820585791383\n"
     ]
    }
   ],
   "source": [
    "test_model(X, y, LassoRegularizer(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc34636",
   "metadata": {
    "id": "3bc34636"
   },
   "source": [
    "As we said before, Lasso Regression prevents overfitting. The MSE loss in this approach is  less than MSE loss in the normal Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63af23a",
   "metadata": {
    "id": "e63af23a"
   },
   "source": [
    "## Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82324fb",
   "metadata": {
    "id": "b82324fb"
   },
   "source": [
    "Ridge Regression uses l2 regularization to prevent model from overfitting. If the model gets complex, the cost function increases. Consequently, the model prevents itself from overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb8b2e28",
   "metadata": {
    "id": "fb8b2e28"
   },
   "outputs": [],
   "source": [
    "class RidgeRegularizer:\n",
    "    \n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def regularize(self, w):\n",
    "        return 2 * self.l * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3e2bb31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3e2bb31",
    "outputId": "30639af4-4623-447d-9999-ede70020643e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.9142553412210468\n"
     ]
    }
   ],
   "source": [
    "test_model(X, y, RidgeRegularizer(0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd9c605c",
   "metadata": {
    "id": "dd9c605c"
   },
   "outputs": [],
   "source": [
    "    \n",
    "class ElasticRegularizer:\n",
    "    \n",
    "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
    "        self.l = l \n",
    "        self.l_ratio = l_ratio\n",
    "\n",
    "    def regularize(self, w):\n",
    "        l1_derivation = self.l * self.l_ratio * np.sign(w)\n",
    "        l2_derivation = self.l * (1 - self.l_ratio) * w\n",
    "        return (l1_derivation + l2_derivation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "PCL3YRH23Te1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCL3YRH23Te1",
    "outputId": "459d8c48-565e-4536-9476-5534effa9957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score 0.9624565535452143\n"
     ]
    }
   ],
   "source": [
    "test_model(X, y, ElasticRegularizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0i4rpOmH6bj_",
   "metadata": {
    "id": "0i4rpOmH6bj_"
   },
   "source": [
    "As we can see, after adding regularizations the r2 scores improve. \n",
    "This means that Lasso, Ridge regressions and elsastic Net prevent the model from overfitting. The do not let the model get complex and fit completely to the training data. Hence, they perform better in the testing phase. \n",
    "But when data is not noisy, it is normal for the ridge, lasso and elastic net to perform worse than linear regression. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
